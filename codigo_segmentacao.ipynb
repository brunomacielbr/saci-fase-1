{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07e53b6-bb0d-4664-9aa4-f6c9d43b570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pacotes necessários para a execução do código\n",
    "\n",
    "import pdal\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from shapely import MultiPoint\n",
    "import alphashape\n",
    "import shapely\n",
    "\n",
    "#Resolução para arquivos raster\n",
    "resolution = 0.5\n",
    "\n",
    "#Carregar dados de LiDAR .laz a partir de uma pasta 'downloads/LiDAR/*.laz'\n",
    "files = glob.glob('downloads/LiDAR/*.laz')\n",
    "\n",
    "#Unir dados de LiDAR num arquivo só\n",
    "!pdal merge {(' ').join(files)} downloads/LiDAR/san_remo.laz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf02e23-0a9f-4122-b8d0-ebc4110caaf9",
   "metadata": {},
   "source": [
    "**Função dem_pipeline(resolution)**\n",
    "- **Objetivo**:\n",
    "    - Recebe dados de LiDAR e gera um Modelo Digital de Elevação (DEM)\n",
    "    - Recebe como valor de entrada a resolução definida anteriormente\n",
    "\n",
    "\n",
    "**1. Leitura dos dados LiDAR**\n",
    "- \"type\": \"readers.las\" → indica que o pipeline vai ler dados LAS (arquivo que armazena dados LiDAR) ou LAZ (versão comprimida)\n",
    "- \"filename\": f'downloads/LiDAR/san_remo.laz' → especifica o caminho para o arquivo LiDAR comprimido (.laz)\n",
    "- \"override-srs\": \"EPSG:31983\" → define o sistema de referência espacial para os dados, nesse caso SIRGAS 2000/UTM zone 23S\n",
    "\n",
    "**2. Filtragem por Classificação**\n",
    "- \"type\": \"filters.range\" → aplica um filtro baseado em intervalos de valores\n",
    "- \"limits\": \"Classification[2:2]\" → filtra os pontos classificados com valor 2, que correspondem aos dados de solo, do terreno, em classificações LiDAR padrão\n",
    "\n",
    "**3. Triangulação de Delaunay**\n",
    "- \"type\": \"filters.delaunay\" → realiza triangulação de Delaunay sobre os pontos filtrados. Isso cria uma malha triangular que cobre os pontos, essencial para a interpolação da superfície do terreno\n",
    "\n",
    "**4. Rasterização da Face**\n",
    "- \"type\": \"filters.faceraster\" → converte a malha triangular resultante em um raster que representa a superfície do terreno\n",
    "- \"resolution\": resolution → define o tamanho da célula do raster a partir do parâmetro de entrada da função\n",
    "\n",
    "**5. Escrita do Raster DEM**\n",
    "- \"filename\": f\"results/DEM/MDT-san_remo-50cm.tiff\" → define o caminho e nome do arquivo de saída\n",
    "    - ATENÇÃO: essas pastas \"results\" e \"DEM\" PRECISAM ser criadas na pasta do teste do código antes de rodá-lo\n",
    "- \"gdaldrive\": \"GTiFF\" → especifica que o formato do arquivo será GeoTIFF, um formato raster georreferenciado\n",
    "- \"type\": \"writers.raster\" → escreve o raster gerado para um arquivo\n",
    "- \"gdalopts\":\"COMPRESS=ZSTD, PREDICTOR=3, BIGTIFF=YES\" → define opções do pacote GDAL\n",
    "    - compress=zstd: usa compressão Zstandard para reduzir o tamanho do arquivo\n",
    "    - predictor=3: otimiza a compressão para dados de ponto flutuante\n",
    "    - bigtiff=yes: permite a criação de arquivos GeoTIFF maiores que 4 GB\n",
    "- \"nodata\": \"0\" → define como valor nulo o que representa ausência de dados no raster\n",
    "- \"data_type\": \"float32\" → float32 indica que os valores do DEM serão armazenados como números de ponto flutuante de 32 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ccf022-23a7-4f3e-8d7e-50c80714c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dem_pipeline(resolution):\n",
    "    return [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"filename\": f'downloads/LiDAR/san_remo.laz',\n",
    "            \"override_srs\": \"EPSG:31983\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.range\",\n",
    "            \"limits\": \"Classification[2:2]\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.delaunay\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.faceraster\",\n",
    "            \"resolution\": resolution\n",
    "        },\n",
    "        {\n",
    "            \"filename\": f\"results/DEM/MDT-san_remo-50cm.tiff\",\n",
    "            \"gdaldriver\": \"GTiff\",\n",
    "            \"type\": \"writers.raster\",\n",
    "            \"gdalopts\": \"COMPRESS=ZSTD, PREDICTOR=3 BIGTIFF=YES\",\n",
    "            \"nodata\": \"0\",\n",
    "            \"data_type\": \"float32\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed089a89-e447-4059-b647-716d4115d346",
   "metadata": {},
   "source": [
    "**Função laz_pipeline(resolution)**\n",
    "- Gera diferentes tipos de arquivos raster e promove clusterização nos pontos do arquivo LiDAR\n",
    "\n",
    "**1. Leitura dos dados LiDAR**\n",
    "- \"type\": \"readers.las\" → indica que o pipeline vai ler dados LAS (arquivo que armazena dados LiDAR) ou LAZ (versão comprimida)\n",
    "- \"filename\": f'downloads/LiDAR/san_remo.laz' → especifica o caminho para o arquivo LiDAR comprimido (.laz)\n",
    "\n",
    "**2. Escrita de um Raster BHM-Z**\n",
    "- **Objetivo**: gerar um raster representando a altura máxima dos pontos classificados como 6 (edificações) na dimensão Z\n",
    "- \"filename\": f\"results/BHM-Z-san_remo.tiff\" → define o formato .tiff como arquivo de saída\n",
    "- \"gdaldriver\":\"GTiff\" → especifica o formato GeoTIFF\n",
    "- \"radius\": f'{resolution * 2 * np.sqrt(2)}' → define o raio para agregação dos pontos. É calculado como resolution * 2 * raiz(2)\n",
    "- \"override_srs\": \"EPSG:31983\" → define sistema de referência espacial para SIRGAS 2000 \n",
    "- \"output_type\":\"max\" → o valor máximo dentro do raio definido será usado para cada célula do raster\n",
    "- \"resolution\":resolution → define a resolução espacial do raster\n",
    "- \"dimension\": \"Z\" → indica que a dimensão de elevação (Z) será utilizada\n",
    "- \"data_type\": \"float32\" → especifica que os valores serão armazenados como números de ponto flutuante de 32 bits\n",
    "- \"type\": \"writers.gdal\" → utiliza GDAL para escrever dados raster\n",
    "- \"gdalopts\":\"COMPRESS=ZSTD, PREDICTOR=3, BIGTIFF=YES\" → define opções do pacote GDAL\n",
    "    - compress=zstd: usa compressão Zstandard para reduzir o tamanho do arquivo\n",
    "    - predictor=3: otimiza a compressão para dados de ponto flutuante\n",
    "    - bigtiff=yes: permite a criação de arquivos GeoTIFF maiores que 4 Gb\n",
    "- \"where\": \"(Classification == 6)\" → filtra os pontos com classificação igual a 6, que representa edificações\n",
    "\n",
    "**3. Cálculo da Altura Acima do Solo (HAG)**\n",
    "- \"type\":\"filters.hag_dem\" → calcula a High Above Ground (HAG), ou seja, a altura dos objetos acima do terreno\n",
    "- \"raster\": f\"results/DEM/MDT-san_remo-50cm.tiff\" → utiliza do DEM (Modelo Digital de Elevação) gerado anteriormente para referenciar o solo\n",
    "\n",
    "**4. Escrita de um Raster BHM**\n",
    "- **Objetivo**: gerar um raster representando a altura das edificações em relação ao solo\n",
    "- \"filename\":f\"results/BHM-san_remo.tiff\" → formato .tiff do arquivo de saída\n",
    "- \"gdaldriver\":\"GTiff\"\n",
    "- \"output_type\":\"max\" → o valor máximo dentro do raio definido será usado para cada célula do raster\n",
    "- \"resolution\": resolution\n",
    "- \"radius\": f'{resolution * 2 * np.sqrt(2)}' → define raio para agregação dos pontos\n",
    "- \"dimension\":\"HeightAboveGround\" → a altura atribuída consiste na altura calculada no passo anterior, ou seja, a altura das edificações com relação ao solo\n",
    "- \"data_type\": \"float32\"\n",
    "- \"type\": \"writers.gdal\"\n",
    "- \"where\": \"(Classification == 6)\" → pontos classificados como edificações\n",
    "- \"override_srs\": \"EPSG:31983\"\n",
    "\n",
    "**5. Filtragem por Classificação**\n",
    "- **Objetivo**: manter apenas os pontos 6, de edificações\n",
    "- \"type\":\"filters.range\"\n",
    "- \"limits\":\"Classification[6:6]\"\n",
    "\n",
    "**6. Redução de densidade de pontos com Voxel Downsize**\n",
    "- **Objetivo**: diminuir a quantidade de pontos para facilitar o processamento subsequente\n",
    "- \"type\":\"filters.voxeldownsize\" → reduz a densidade da nuvem de pontos usando uma grade voxel. Voxel é um ponto tridimensional em uma grade cartesiana. O termo vem da junção das palavras \"volume\" e \"pixel\"\n",
    "- \"cell\":0.5 → define o tamanho da célula (confirmar se são em metros)\n",
    "- \"mode\":\"center\" → seleciona o ponto central dentro de cada voxel para representar o voxel reduzido\n",
    "\n",
    "**7. Clustering com DBSCAN**\n",
    "- **Objetivo**: identificar cluster de pontos usando o algortimo DBSCAN\n",
    "- \"type\":\"filters.dbscan\" → aplica o algoritmo DBSCAN para identificar cluster na nuvem de pontos\n",
    "- \"min_points\":5 → número mínimo de pontos para formar um cluster\n",
    "- \"eps\": (resolution + 0.10) * np.sqrt(2) → distância máxima entre dois pontos para que sejam considerados no mesmo cluster\n",
    "- \"dimensions\":\"X,Y,Z\" → utiliza as três dimensões espaciais x, y e z para o clustering\n",
    "\n",
    "**8. Transporte de dimensões com Ferry**\n",
    "- **Objetivo**: ajustar os valores de altura após o clustering\n",
    "- \"type\":\"filters.ferry\" → transfere os valores de uma dimensão para outra\n",
    "- \"dimensions\":\"HeightAboveGround => Z\" → substitui os valores da coordenada Z (altura com relação ao nível do mar) pela altura com relação ao solo\n",
    "\n",
    "**9. Escrita dos pontos clusterizados**\n",
    "- **Objetivo**: salva a nuvem de pontos processada com informações clusterizadas\n",
    "- \"type\":\"writers.las\" → escreve a nuvem de pontos com clusterização de volta para um arquivo .las\n",
    "- \"filename\":f\"results/Cluster-san_remo.laz\" → formato do arquivo de saída\n",
    "- \"extra_dims\": \"all\" → o \"all\" inclui todas as dimensões adicionais geradas durante o processamento, como ClusterID do DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d150cb-2653-4b75-a60b-a71ba0cc0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laz_pipeline(resolution):\n",
    "    return [\n",
    "        {\n",
    "            \"type\":\"readers.las\",\n",
    "            \"filename\":f\"downloads/LiDAR/san_remo.laz\"\n",
    "        },\n",
    "        {\n",
    "            \"filename\":f\"results/BHM-Z-san_remo.tiff\",\n",
    "            \"gdaldriver\":\"GTiff\",\n",
    "            \"radius\": f'{resolution * 2 * np.sqrt(2)}',\n",
    "            \"override_srs\": \"EPSG:31983\",\n",
    "            \"output_type\":\"max\",\n",
    "            \"resolution\":resolution,\n",
    "            \"dimension\": \"Z\",\n",
    "            \"data_type\": \"float32\",\n",
    "            \"type\": \"writers.gdal\",\n",
    "            \"gdalopts\":\"COMPRESS=ZSTD, PREDICTOR=3, BIGTIFF=YES\",\n",
    "            \"where\": \"(Classification == 6)\",\n",
    "        },\n",
    "        {\n",
    "            \"type\":\"filters.hag_dem\",\n",
    "            \"raster\": f\"results/DEM/MDT-san_remo-50cm.tiff\"\n",
    "        },\n",
    "        {\n",
    "            \"filename\":f\"results/BHM-san_remo.tiff\",\n",
    "            \"gdaldriver\":\"GTiff\",\n",
    "            \"output_type\":\"max\",\n",
    "            \"resolution\": resolution,\n",
    "            \"radius\": f'{resolution * 2 * np.sqrt(2)}',\n",
    "            \"dimension\":\"HeightAboveGround\",\n",
    "            \"data_type\": \"float32\",\n",
    "            \"type\": \"writers.gdal\",\n",
    "            \"where\": \"(Classification == 6)\",\n",
    "            \"override_srs\": \"EPSG:31983\"\n",
    "        },\n",
    "        {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification[6:6]\"\n",
    "        },\n",
    "        {\n",
    "            \"type\":\"filters.voxeldownsize\",\n",
    "            \"cell\":0.5,\n",
    "            \"mode\":\"center\"\n",
    "        },\n",
    "        {\n",
    "            \"type\":\"filters.dbscan\",\n",
    "            \"min_points\":9,\n",
    "            \"eps\": ((resolution * np.sqrt(2)) + 0.1),\n",
    "            \"dimensions\":\"X,Y,Z\"\n",
    "        },\n",
    "        {\n",
    "            \"type\":\"filters.ferry\",\n",
    "            \"dimensions\":\"HeightAboveGround => Z\"\n",
    "        },\n",
    "        {\n",
    "            \"type\":\"writers.las\",\n",
    "            \"filename\":f\"results/Cluster-san_remo.laz\",\n",
    "            \"extra_dims\": \"all\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10574ff-c1e1-4853-952b-c70d1173a0e4",
   "metadata": {},
   "source": [
    "**Estrutura geral do código**\n",
    "1. Definição de Dicionários de Agregação e Renomeação de Colunas\n",
    "2. Processamento com Pipelines PDAL (DEM e LAZ)\n",
    "3. Manipulação de Dados com Pandas\n",
    "4. Agregação e Conversão para GeoDataFrame\n",
    "5. Exportação dos Resultados\n",
    "\n",
    "**1. Definição de Dicionários de Agregação e Renomeação de Colunas**\n",
    "- **Objetivo**: definir como os dados serão agregados após o agrupamento em cluster e como as colunas resultantes serão renomeadas para melhor legibilidade\n",
    "- agg → é um dicionário que define como os dados serão agregados após o agrupamento. As chaves são os nomes das colunas e os valores especificam as operações de agregação a serem aplicadas\n",
    "    - 'coords': list → agrupa as coordenadas em listas\n",
    "    - 'Z': ['count', 'median', 'sum'] → conta o número de pontos (count), calcula a mediana (median) e soma (sum) os valores de Z\n",
    "    - 'Intensity': 'median' → calcula a mediana do atributo\n",
    "    - 'Red': 'median' \n",
    "    - 'Green': 'median' \n",
    "    - 'Blue': 'median' \n",
    "\n",
    "- columns → é um dicionário que mapeia os nomes das colunas resultantes após a agregação para nomes mais legíveis\n",
    "    - ('coords', 'list'): 'coords' → renomeia ('coords', 'list') para somente 'coords'\n",
    "    - ('Z', 'count'): 'count' \n",
    "    - ('Z', 'median'): 'z_median' \n",
    "    - ('Z', 'sum'): 'z_sum' \n",
    "    - ('Intensity', 'median'): 'intensity_median' \n",
    "    - ('Red', 'median'): 'red_median' \n",
    "    - ('Green', 'median'): 'green_median'  \n",
    "    - ('Blue', 'median'): 'blue_median' \n",
    "\n",
    "\n",
    "**2. Processamento com Pipelines PDAL (DEM e LAZ)**\n",
    "- **Objetivo**: executar as funções detalhadas nos dois códigos anteriores\n",
    "- print('processando') → exibe uma mensagem indicando que o processamento dos dados está iniciando\n",
    "- dem = dem_pipeline(resolution) → chama a função *dem_pipeline*\n",
    "    - pipeline = pdal.Pipeline(json.dumps(dem)) → cria um objeto Pipeline da biblioteca PDAL, passando o Pipeline para o formato JSON\n",
    "    - n_points = pipeline.execute() → executa o pipeline PDAL, processando os dados conforme definido na função *dem_pipeline*\n",
    "    - print(f'Pipeline selected {n_points} points') → exibe o número de pontos processados pelo pipeline\n",
    "- laz = laz_pipeline(resolution) → chama a função *laz_pipeline*\n",
    "    - pipeline = pdal.Pipeline(json.dumps(laz)) → cria um objeto Pipeline da biblioteca PDAL, passando o Pipeline para o formato JSON\n",
    "    - n_points = pipeline.execute() → executa o pipeline PDAL\n",
    "    - print(f'Pipeline selected {n_points} points') → exibe o número de pontos processados pelo pipeline LAZ\n",
    "\n",
    "\n",
    "**3. Manipulação de Dados com Pandas**\n",
    "- **Objetivo**: converter os dados processados em um DataFrame, adicionar coordenadas, remover duplicatas e filtrar pontos com valores de Z específicos\n",
    "- arr = pipeline.arrays[0] → obter uma array de pontos resultante da execução do pipeline PDAL\n",
    "- df = pd.DataFrame(arr) → converter a array de pontos em um DataFrame do Pandas para facilitar a manipulação dos dados\n",
    "- df.loc[:, 'coords'] = list(np.dstack([df.X, df.Y])[0])\n",
    "    - np.dstack([df.X, df.Y]) → empilhar as colunas X e Y ao longo da terceira dimensão, criando pares de coordenadas\n",
    "    - list(...[0]) → converte o resultado em uma lista de coordenadas\n",
    "    - df.loc[:, 'coords'] = ... → adicionar uma nova coluna 'coords' ao DataFrame contendo as coordenadas (X, Y) para cada ponto\n",
    "- df.set_index(['X', 'Y']).loc[:, 'Z'] = df.groupby(['X', 'Y']).agg({'Z': 'max'})\n",
    "    - df.set_index(['X', 'Y']) → definir as colunas X e Y como índices do DataFrame\n",
    "    - df.groupby(['X', 'Y']).agg({'Z': 'max'}) → agrupar os dados por X e Y e calcular o valor máximo de Z para cada grupo\n",
    "    - df.loc[:, 'Z'] = ... → atualizar a coluna Z no DataFrame com os valores máximos calculados\n",
    "- df.drop_duplicates(subset=['X', 'Y'], keep='last', inplace=True) → remover duplicatas de pontos com as mesmas coordenadas X e Y, mantendo apenas o último ponto\n",
    "- df = df[(df.Z > 2.0) & (df.Z < 200.0)].reset_index()\n",
    "    - df[(df.Z > 2.0) & (df.Z < 200.0)] → filtrar os pontos para manter apenas aqueles com Z entre 2.0 e 200.0\n",
    "    - .reset_index() → resetar os índices do DataFrame após o filtro\n",
    "  \n",
    "**4. Agregação e Conversão para GeoDataFrame**\n",
    "- **Objetivo**: agrupar os dados por ClusterID, aplicar as agregações definidas, renomear colunas, criar geometrias MultiPoint e converter para um GeoDataFrame\n",
    "- df_agg = df[df.ClusterID > 0].groupby('ClusterID').agg(agg)\n",
    "    - df[df.ClusterID > 0] → filtrar os pontos para manter apenas aqueles que pertencem a um cluster (assumindo que ClusterID > 0 indica pertencimento a um cluster)\n",
    "    - .groupby('ClusterID').agg(agg) → agrupar os dados pelo ClusterID e aplicar as operações de agregação definidas no dicionário agg\n",
    "- df_agg.columns = df_agg.columns.to_flat_index() → achatar os índices de múltiplos níveis resultantes da agregação para facilitar o acesso às colunas\n",
    "- df_agg.rename(columns=columns, inplace=True) → renomear as colunas do DataFrame de acordo com o mapeamento definido no dicionário columns\n",
    "- df_agg.loc[:, 'geometry'] = df_agg.coords.apply(MultiPoint)\n",
    "    - df_agg.coords.apply(MultiPoint) → converter a lista de coordenadas em objetos MultiPoint do GeoPandas\n",
    "    - df_agg.loc[:, 'geometry'] = ... → adicionar uma nova coluna 'geometry' contendo os objetos MultiPoint\n",
    "- gdf_agg = gpd.GeoDataFrame(df_agg) → converter o DataFrame Pandas em um GeoDataFrame do GeoPandas, que suporta operações geoespaciais\n",
    "- gdf_agg.set_crs(epsg=31983, inplace=True) → definir o Sistema de Referência de Coordenadas (CRS) do GeoDataFrame para EPSG:31983, que é o SIRGAS 2000 / UTM zone 23S\n",
    "\n",
    "**5.Exportação dos Resultados**\n",
    "- **Objetivo**: exportar o GeoDataFrame final para um arquivo GeoPackage, o que facilita o uso em softwares como o QGis e outras análises geoespaciais\n",
    "- gdf_agg.drop(columns=['coords']).to_file(f'results/san_remo-multipoint.gpkg', driver='GPKG')\n",
    "    - gdf_agg.drop(columns=['coords']) → remover a coluna 'coords' do GeoDataFrame, pois as informações de geometria já estão contidas na coluna 'geometry'. \n",
    "    - to_file → método do GeoPandas para exportar o GeoDataFrame para um arquivo\n",
    "    - f'results/san_remo-multipoint.gpkg' → caminho e nome do arquivo de saída. gpkg indica que o formato será GeoPackage.\n",
    "    - driver='GPKG' → especificar que o formato de saída é GeoPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f6b584f-f080-40da-bc9f-7a6ec5adc1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processando\n",
      "Pipeline selected 6349330 points\n",
      "Pipeline selected 1866807 points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39996\\1348440937.py:38: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df.set_index(['X', 'Y']).loc[:, 'Z'] = df.groupby(['X', 'Y']).agg({'Z':'max'})\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39996\\1348440937.py:38: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  df.set_index(['X', 'Y']).loc[:, 'Z'] = df.groupby(['X', 'Y']).agg({'Z':'max'})\n",
      "Cannot find header.dxf (GDAL_DATA is not defined)\n"
     ]
    }
   ],
   "source": [
    "agg = {\n",
    "    'coords':list,  \n",
    "    'Z':['count', 'median', 'sum'], \n",
    "    'Intensity':'median', \n",
    "    'Red':'median',\n",
    "    'Green':'median',\n",
    "    'Blue':'median'  \n",
    "}\n",
    "\n",
    "columns = {\n",
    "    ('coords', 'list'):'coords',\n",
    "    ('Z', 'count'):'count',\n",
    "    ('Z', 'median'):'z_median',\n",
    "    ('Z', 'sum'):'z_sum',\n",
    "    ('Intensity', 'median'):'intensity_median',\n",
    "    ('Red', 'median'):'red_median',\n",
    "    ('Green', 'median'):'green_median',\n",
    "    ('Blue', 'median'):'blue_median',\n",
    "}\n",
    "\n",
    "print('processando')\n",
    "\n",
    "dem = dem_pipeline(resolution)\n",
    "pipeline = pdal.Pipeline(json.dumps(dem))\n",
    "\n",
    "laz = laz_pipeline(resolution)\n",
    "pipeline = pdal.Pipeline(json.dumps(laz))\n",
    "\n",
    "\n",
    "arr = pipeline.arrays[0]\n",
    "df = pd.DataFrame(arr)\n",
    "df.loc[:, 'coords'] = list(np.dstack([df.X, df.Y])[0])\n",
    "df.set_index(['X', 'Y']).loc[:, 'Z'] = df.groupby(['X', 'Y']).agg({'Z':'max'})\n",
    "df.drop_duplicates(subset=['X', 'Y'], keep='last', inplace=True)\n",
    "df = df[(df.Z > 2.0) & (df.Z < 200.0)].reset_index()\n",
    "\n",
    "df_agg = df[df.ClusterID > 0].groupby('ClusterID').agg(agg)\n",
    "df_agg.columns = df_agg.columns.to_flat_index()\n",
    "df_agg.rename(columns=columns, inplace=True)\n",
    "df_agg.loc[:, 'geometry'] = df_agg.coords.apply(MultiPoint)\n",
    "gdf_agg = gpd.GeoDataFrame(df_agg)\n",
    "gdf_agg.set_crs(epsg=31983, inplace=True)\n",
    "gdf_agg.drop(columns=['coords']).to_file(f'results/san_remo-multipoint.gpkg', driver='GPKG')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f802c6-689e-43b4-88f3-7f9b826d141e",
   "metadata": {},
   "source": [
    "**Processamentos finais**\n",
    "\n",
    "**Estrutura geral do código**\n",
    "1. Remoção da coluna 'coords'\n",
    "2. Criação de Concave Hulls (envoltórias côncavas) para cluster significativos\n",
    "3. Armazenamento das geometrias originais\n",
    "4. Substituição das geometrias pelo Concave Hull\n",
    "5. Exportação das geometrias poligonais\n",
    "6. Restauração das geometrias originais\n",
    "\n",
    "**1. Remoção da coluna 'coords'**\n",
    "- **Objetivo**: remover a coluna 'coords' do DataFrame\n",
    "- gdf_agg.drop(columns=['coords']) → a função 'drops' remove a coluna 'coords' do GeoDataFrame gdf_agg\n",
    "\n",
    "\n",
    "**2. Criação de Concave Hulls (envoltórias côncavas) para cluster significativos**\n",
    "- **Objetivo**: filtrar cluster com 16 ou mais pontos e aplicar uma função para criar concave hulls para esses clusters\n",
    "- ashapes = gdf_agg.loc[df_agg.loc[:, 'count'] >= 16].coords.apply( lambda x: shapely.concave_hull(MultiPoint(x), ratio=0.1, allow_holes=Tru))\n",
    "    - gdf_agg.loc[df_agg.loc[:, 'count'] >= 16] → selecionar os clusters (gdf_agg) nos quais a contagem de pontos (count) é maior ou igual a 16\n",
    "    - coords → coluna que contém listas de coordenas [X, Y] para cada cluster\n",
    "    - apply(lambda x: ...) → aplicar uma função lambda a cada lista de coordenadas. Uma função lambda é uma maneira de escrever uma função em uma única expressão, sem a necessidade de defini-la explicitamente\n",
    "    - Multipoint(x) → criar um objeto MultiPoint do Shapely a partir das coordenadas\n",
    "    - shapely.concave_hull(...) → criar a envoltória côncava dos pontos\n",
    ")\r\n",
    "\n",
    "\n",
    "\n",
    "**3. Armazenamento das geometrias originais**\n",
    "- **Objetivo**: salvar temporariamente as geometrias originais para restaurá-las após a expor- geometry = gdf_agg.geometry → salvar temporariamente as geometrias originais do GeoDataFrame gdf_agg para restaurá-las posteriormente\n",
    "  tação\n",
    "\n",
    "\n",
    "\n",
    "**4. Substituição das geometrias pelo Concave Hull**\n",
    "- **Objetivo**: atualizar as geometrias do GeoDataFrame para as concave hulls criadas, apenas para os clusters fil- gdf_agg.geometry = ashapes → substituir a coluna 'geometry' do GeoDataFrame gdf_agg pelas geometrias de concave hull geradas para os cluster filtrados. Para os cluster com 16 ou mais pontos, as geometrias são representadas por concave hulls, enquanto os demais permanecem inalterados\r\n",
    "rados\n",
    "\n",
    "\n",
    "\n",
    "**5. Exportação das geometrias poligonais**\n",
    "- **Objetivo**: exportar o GeoDataFrame modificado para um arquivo GeoPackage contendo os polígonos das áreas ag- gdf_agg.drop(columns=['coords']).to_file(f'results/san_remo-multipoligono.gpkg', driver='GPKG')\r",
    "    - drop(columns=['coords']) → remover a coluna 'coords' do GeoDataFrame\n",
    "    - .to_file(..., driver='GPKG') → exportar o GeoDataFrame para um arquivo GeoPackage (.gpkg), um formato de contêiner para dados geoespaciais, que suporta múltiplas camadas e formatos de dados\n",
    "rupadas\n",
    "\n",
    "\n",
    "\n",
    "**6. Restauração das geometrias originais**\n",
    "- **Objetivo**: reverter as geometrias para as suas formas originais (MultiPoint), permitindo que o GeoDataFrame continue a ser usado com suas geometrias iniciais para operações- gdf_agg.geometry = geometry → restaurar a coluna 'geometry' original do GeoDataFrame gdf_agg, revertendo as geometrias de concave hull para as geometrias iniciais (MultiPoint). Isso permite que o GeoDataFrame gdf_agg continue sendo utilizado com suas geometrias originais para operações futuras, sem a necessidade de manter as alterações temporárias feitas para exportação.\r\n",
    " futuras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a1c6ad8-9602-4888-a816-969ae5111eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_agg.drop(columns=['coords'])\n",
    "\n",
    "ashapes = gdf_agg.loc[df_agg.loc[:, 'count'] >= 16].coords.apply(lambda x: shapely.concave_hull(MultiPoint(x), ratio=0.1, allow_holes=True))\n",
    "\n",
    "gdf_agg.geometry = ashapes\n",
    "\n",
    "gdf_agg.drop(columns=['coords']).to_file(f'results/san_remo-multipoligono.gpkg', driver='GPKG')\n",
    "\n",
    "gdf_agg.geometry = geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ac47f-16bc-48e8-9530-2ae993d38716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pdalpy)",
   "language": "python",
   "name": "pdalpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
